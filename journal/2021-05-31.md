# Monday, May 31, 2021

## Todos:

DeepMind
  * need to confirm that if you remove attention, this hurts performance...
    * attention is important for zero-shot task composition (i.e. new task argument)
    * is attention important for breaking up sensory stream?


Papers:
* Learning to represent state with Dynamics Schemata (Machine Learning)
  * Me, Murray, Kyriacos, Felix, Andrew
* Towards a form definition of Schemata (Psychology)
  * Me, Abel, Rick
* Why Schemata enable generalization? (RL Theory)
  * use DeepMind work as empirical evidence to guide results?
  * Me & Abel (Satinder wouldn't like this)
