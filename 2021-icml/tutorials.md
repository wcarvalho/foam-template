---
tags: [continual-learning]
---


# [Continual Learning with Deep Architectures](https://icml.cc/virtual/2021/tutorial/10833)


### Supervised Learning
2021 superivsed learning survey: [A continual learning survey: Defying forgetting in classification tasks](https://arxiv.org/pdf/1909.08383.pdf)

![[Pasted image 20210817133640.png]]
- continual vs. others (all others ignore catastrophic forgetting)
	- multitask: forced order of learning data
	- transfer: don't care about learning initial tasks
	- domain-adaptation: don't care about source performance
	- meta-learning: mainly test few-shot learning performance


### Reinforcement Learning
2021 RL survey: [Towards Continual Reinforcement Learning: A Review and Perspectives](https://arxiv.org/abs/2012.13490)

Kinds of non-stationarity:
* Passive non-stationarity (environment is changing)
* Active non-stationarity (other actors acting in environment)
* Hybrid: (both)



# Unsupervised Learning for RL
